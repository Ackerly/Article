# QUIC和HTTP3队头阻塞的细节
**什么是队头阻塞**  
> 当单个（慢）对象阻止其他/后续的对象前进时

现实生活中一个很好的比喻就是只有一个收银台的杂货店。一个顾客买了很多东西，最后会耽误排在他后面的人，因为顾客是以先进先出（First In, First Out）的方式服务的。另一个例子是只有单行道的高速公路。在这条路上发生一起车祸，可能会使整个通道堵塞很长一段时间。因此，即使是在“头部（head）”一个单一的问题可以“阻塞（block）”整条“线（line）”。  
## HTTP/1.1 的队头阻塞
HTTP 方面本身很简单：它只是在明文文件内容或“有效荷载”（payload）前面直接添加一些文本“headers”（红色）。然后，头（Headers）+ 有效荷载（payload）被传递到底层 TCP（橙色），以便真实传输到客户端。对于这个例子，假设我们不能将整个文件放入一个 TCP 包中，并且必须将它分成两部分。  
当浏览器请求style.css时发生了什么:  
当script.js的响应传输之后，发送style.css（紫色）。style.css的头部（headers）和内容只是附加在 JavaScript（JS）文件之后。接收者使用Content-Length header 来知道每个响应的结束位置和另一个响应的开始位置（在简化示例中，script.js是1000字节，而style.css只有600字节）。
假设 JS 文件比 CSS 大得多（比如说 1MB 而不是 1KB）。这种情况下，在下载整个JS文件之前，CSS 必须等待，尽管它要小得多，其实可以更早地解析/使用。  
可以看到这是一个队头阻塞问题的例子！现在你可能会想：这很容易解决！只需让浏览器在JS文件之前请求CSS文件！然而，至关重要的是，浏览器无法预先知道这两个文件中的哪一个在请求时会成为更大的文件。这是因为没有办法在HTML中指明文件有多大（类似这样的东西很不错，HTML工作组：<img src="thisisfine.jpg" size="15000" />）。  
“真正”解决方案是采用多路复用（multiplexing）。如果我们可以将每个文件的有效荷载（header）分成更小的片（pieces）或“块”（chunks），我们就可以在网络上混合或“交错”（interleave）这些块：为 JS 发送一个块，为 CSS 发送一个块，然后再发送另一个用于 JS，等等，直到文件被下载为止。使用这种方法，较小的CSS文件将更早地下载（并且可用），同时只将较大的JS文件延迟一点。  
由于协议存在一些基础的限制，这种多路复用在 HTTP/1.1 中是不可能的。这里的主要问题是 HTTP/1.1 是一个纯文本协议，它只在有效荷载（payload）的前面附加头（headers）。它不会进一步区分单个（大块）资源与其他资源。  
用一个例子来:  
说浏览器开始分析script.js并期望后面有1000个字节（Content-Length）的有效荷载。但是，它只接收450个 JS 字节（第一个块），然后开始读取sytle.css的头部。它最终将 CSS 头部和第一个 CSS 块解释为JS的一部分，因为这两个文件的有效荷载和头都是纯文本。更糟糕的是，它在读取1000个字节后停止，直到第二个script.js块的一半。此时，它看不到有效的新报头，必须删除 TCP 数据包3（packet 3）的其余部分。然后浏览器传递它认为的内容script.js到JS解析器，它失败了因为不是有效的 JavaScrip  
同样有一个简单的解决方案：让浏览器查找HTTP/1.1 {statusCode} {statusString}\n模式来查看新的头块何时开始。这可能适用于 TCP 数据包2（packet 2），但在数据包3（packet 3）中会失败：浏览器如何知道绿色的script.js块在哪里结束和紫色style.css块从哪里开始？  
这是 HTTP/1.1 协议设计方式的一个基础限制。如果您只有一个 HTTP/1.1 连接，那么在您切换到发送新资源之前，必须完整地传输资源响应。如果前面的资源创建缓慢（例如，从数据库查询动态生成的index.html）或者，如上所述，如果前面的资源很大。这些问题可能会引起队头阻塞问题。  
这就是为什么浏览器开始为 HTTP/1.1 上的每个页面加载打开多个并行 TCP 连接（通常为6个）。这样，请求可以分布在这些单独的连接上，并且不再有队头阻塞。也就是说，除非每页有超过6个资源…这当然是很常见的。这就是在多个域名上“分片”（sharding）资源的实践(img.mysite.com, static.mysite.com, 等）和 CDN 的由来。由于每个单独的域名有6个连接，浏览器将为每个页面加载总共打开 30-ish 个 TCP 连接。这是可行的，但有相当大的开销：建立一个新的 TCP 连接可能是昂贵的（例如在服务器上的状态和内存方面，以及设置 TLS 加密的计算），并且需要消耗一些时间（特别是对于 HTTPS 连接，因为 TLS 需要自己的握手）。  
## HTTP/2（基于 TCP）的队头阻塞
HTTP/1.1 有一个队头阻塞问题，一个大的或慢的响应会延迟后面的其他响应。这主要是因为协议本质上是纯文本的，在资源块（resource chunks）之间不使用分隔符。作为一种解决办法，浏览器打开许多并行TCP连接，这既不高效，也不可扩展。  
HTTP/2 的目标非常明确：能够回到单个 TCP 连接，解决队头阻塞问题。换一种说法：我们希望能够正确地复用资源块（resource chunks）。这在 HTTP/1.1 中是不可能的，因为没有办法分辨一个块属于哪个资源，或者它在哪里结束，另一个块从哪里开始。HTTP/2 非常优雅地解决了这一问题，它在资源块之前添加了帧（frames）。  
HTTP/2 在每个块前面放置一个所谓的数据帧（DATA frame）。这些数据帧主要包含两个关键的元数据。首先：下面的块属于哪个资源。每个资源的“字节流（bytestream）”都被分配了一个唯一的数字，即流id（stream id）。第二：块的大小是多少。协议还有许多其他帧类型，图5也显示了头部帧（HEADERS frame）。这再次使用流id（stream id）来指出这些头（headers）属于哪个响应，这样甚至可以将头（headers）从它们的实际响应数据中分离出来。  
使用这些帧，HTTP/2 确实允许在一个连接上正确地复用多个资源  
浏览器现在可以完美地处理这种情况,它首先处理script.js的头部帧（HEADERS frame），然后是第一个JS块的数据帧（DATA frame）。从数据帧（DATA frame）中包含的块长度来看，浏览器知道它只延伸到 TCP 数据包1的末尾，并且需要从 TCP 数据包2开始寻找一个全新的帧。在那里它确实找到了style.css的头（HEADERS）， 下一个数据帧（DATA frame）含有与第一个数据帧（1）不同的流 id（2），因此浏览器知道这属于不同的资源。同样的情况也适用于 TCP 数据包3，其中数据帧（DATA frame）流 id 用于将响应块“解复用”（de-multiplex）到正确的资源“流”（streams）。  
通过“framing”单个消息，HTTP/2 比 HTTP/1.1 更加灵活。它允许在单个 TCP 连接上通过交错排列块来多路传输多个资源。它还解决了第一个资源缓慢时的队头阻塞问题：而不必等待查询数据库生成的index.html，服务器可以在等待index.html时开始发送其他资源。  
HTTP/2 有更多的自由：  
- 公平多路复用（例如两个渐进的 JPEGs）
- 加权多路复用（2是1的两倍
- 反向顺序调度（例如2是密钥服务器推送的资源
- 反向顺序调度（例如2是密钥服务器推送的资源

使用哪种方法是由 HTTP/2 中所谓的“优先级（prioritization）”系统驱动的，所选择的方法对Web 性能有很大的影响。  
## TCP 队头阻塞
HTTP/2 只解决了 HTTP 级别的队头阻塞，我们可以称之为“应用层”队头阻塞。然而，在典型的网络模型中，还需要考虑下面的其他层。  
HTTP 位于顶层，但首先由安全层的 TLS 支持,然后接着再由传输层的 TCP 传输。这些协议中的每一层都用一些元数据包装来自其上一层的数据。例如，在我们的 HTTP(S) 数据中预先加上 TCP 包头（packet header），然后将其放入 IP 包等，这样就可以在协议之间实现相对简洁的分离。这反过来又有利于它们的可重用性：像 TCP 这样的传输层协议不必关心它正在传输什么类型的数据（可以是 HTTP，也可以是 FTP，也可以是 SSH，谁知道呢），而且 IP 对于 TCP 和 UDP 都能很好地工作。  
虽然我们和浏览器都知道我们正在获取 JavaScript 和 CSS 文件，但 HTTP/2 不需要知道这一点。它只知道它在使用来自不同资源流 id （stream id）的块。然而，TCP 甚至不知道它在传输 HTTP！TCP 所知道的就是它被赋予了一系列字节，它必须从一台计算机传输另一台计算机。为此，它使用特定最大大小（maximum size）的数据包，通常大约为1450字节。每个数据包只跟踪它携带的数据的那一部分（字节范围），这样原始数据就可以按照正确的顺序重建。  
这两个层之间的透视图是不匹配的：HTTP/2 可以看到多个独立的资源字节流（bytestream），而 TCP 只看到一个不透明的字节流（bytestreams）。图7的TCP数据包3就是一个例子：TCP 只知道它正在传输的任何内容的字节 750 到字节1 599。另一方面，HTTP/2 知道数据包3中实际上有两个独立资源的两个块。（注意：实际上，每个 HTTP/2 帧（如 DATA 和 HEADERS）的大小也有几个字节。为了简单起见，我没有计算额外的开销或这里的 HEADERS 帧，以使数字更直观。）  
再次思考下：如果 TCP 数据包2在网络中丢失，但数据包1和数据包3已经到达，会发生什么情况？请记住，TCP并不知道它正在承载 HTTP/2，只知道它需要按顺序传递数据。因此，它知道数据包1的内容可以安全使用，并将这些内容传递给浏览器。然而，它发现数据包1中的字节和数据包3中的字节（放数据包2 的地方）之间存在间隙，因此还不能将数据包3传递给浏览器。TCP 将数据包3保存在其接收缓冲区（receive buffer）中，直到它接收到数据包2的重传副本（这至少需要往返服务器一次），之后它可以按照正确的顺序将这两个数据包都传递给浏览器。换个说法：丢失的数据包2 队头阻塞（HOL blocking）数据包3！  
TCP 数据包2只携带流id 2（CSS文件）的数据，数据包3同时携带流1（JS文件）和流2的数据。在 HTTP 级别，我们知道这两个流是独立的，并且由数据帧（DATA frame）清楚地描述出来。因此，理论上我们可以完美地将数据包3传递给浏览器，而不必等待数据包2到达。浏览器将看到流id为1的数据帧，并且能够直接使用它。只有流2必须被挂起，等待数据包2的重新传输。这将比我们从 TCP 的方式中得到的效率更高，TCP 的方式最终会阻塞流1和流2。  
另一个例子是数据包1丢失，但是接收到2和3的情况。TCP将再次阻止数据包2和3，等待1。但是，我们可以看到，在HTTP/2级别，流2的数据（CSS文件）完全存在于数据包2和3中，不必等待数据包1的重新传输。浏览器本可以完美地解析/处理/使用 CSS 文件，但却被困在等待 JS 文件的重新传输。  
如果我们仍然有 TCP 队头阻塞，为什么还要使用HTTP/2 呢？主要原因是虽然数据包丢失确实发生在网络上，但还是比较少见的。特别是在有线网络中，包丢失率只有 0.01%。即使是在最差的蜂窝网络上，在现实中，也很少看到丢包率高于2%。这与数据包丢失和抖动（网络中的延迟变化）通常是突发性的这一事实结合在一起的。包丢失率为2%并不意味着每100个包中总是有2个包丢失（例如数据包 42 和 96）。实际上，可能更像是在总共500个包中丢失10个连续的包（例如数据包255到265）。这是因为数据包丢失通常是由网络路径中的路由器内存缓冲区暂时溢出引起的，这些缓冲区开始丢弃无法存储的数据包。不过，细节在这里并不重要（如果你想知道更多，可以在其他地方找到）。重要的是：是的，TCP 队头阻塞是真实存在的，但是它对 Web 性能的影响要比HTTP/1.1 队头阻塞小得多，HTTP/1.1 队头阻塞几乎可以保证每次都会遇到它，而且它也会受到 TCP 队头阻塞的影响！  
当比较单个连接上的 HTTP/2 和单个连接上的 HTTP/1.1 时，这个基本上是真的。正如我们之前所看到的，实际上它并不是这样工作的，因为 HTTP/1.1 通常会打开多个连接。这使得 HTTP/1.1 不仅在一定程度上减轻了 HTTP 级别，而且减轻了 TCP 级别的队头阻塞。因此，在某些情况下，单个连接上的 HTTP/2 很难比6个连接上的 HTTP/1.1 快，甚至与 HTTP/1.1 一样快。这主要是由于 TCP 的“拥塞控制”（congestion control）机制。  
HTTP/2 目前部署在浏览器和服务器中，在大多数情况下通常与 HTTP/1.1 一样快或略快。在我看来，这部分是因为网站在优化 HTTP/2 方面做得更好，部分原因是浏览器仍然经常打开多个并行 HTTP/2 连接（要么是因为站点仍然在不同的服务器上共享资源，要么是因为与安全相关的副作用），从而使两者兼得。
也有一些情况（特别是在数据包丢失率较高的低速网络上），6个连接的 HTTP/1.1 仍然比一个连接的 HTTP/2 更为出色，这通常是由于 TCP 级别的队头阻塞问题造成的。正是这个事实极大地推动了新的 QUIC 传输协议的开发，以取代 TCP。  
## HTTP/3（基于 QUIC）的队头阻塞
- HTTP/1.1 有队头阻塞，因为它需要完整地发送响应，并且不能多路复用它们
- HTTP/2 通过引入“帧”（frames）标识每个资源块属于哪个“流”（stream）来解决这个问题
- 然而，TCP 不知道这些单独的“流”（streams），只是把所有的东西看作一个大流（1 big stream）
- 如果一个 TCP 包丢失，所有后续的包都需要等待它的重传，即使它们包含来自不同流的无关联数据。TCP 具有传输层队头阻塞。

解决方案很简单：我们“只是”需要让传输层知道不同的、独立的流！这样，如果一个流的数据丢失，传输层本身就知道它不需要阻塞其他流。  
选择的替代方法是以 QUIC 的形式实现一个全新的传输层协议。为了使 QUIC 现实中可以部署在因特网上，它运行在不可靠的 UDP 协议之上。然而，非常重要的是，这并不意味着 QUIC 本身也是不可靠的！在许多方面，QUIC 应该被看作是一个 TCP 2.0。它包括 TCP 的所有特性（可靠性、拥塞控制、流量控制、排序等）的最佳版本，以及更多其他特性。QUIC还完全集成了TLS（参见图6），并且不允许未加密的连接。因为 QUIC 与 TCP 如此不同，这也意味着我们不能仅仅在其上运行   
让 QUIC 知道不同的流（streams）是非常简单的。QUIC 受到 HTTP/2 帧方式（framing-approach）的启发，还添加了自己的帧（frames）；在本例中是流帧（STREAM frame）。流id（stream id）以前在 HTTP/2 的数据帧（DATA frame）中，现在被下移到传输层的 QUIC 流帧（STREAM frame）中。这也说明了如果我们想使用 QUIC，我们需要一个新版本的 HTTP 的原因之一：如果我们只在 QUIC 之上运行 HTTP/2，那么我们将有两个（可能冲突的）“流层”（stream layers）。相反，HTTP/3 从 HTTP 层删除了流的概念（它的数据帧（DATA frames）没有流id），而是重新使用底层的 QUIC 流。  
注意：这并不意味着 QUIC 突然知道 JS 或 CSS 文件，甚至知道它正在传输 HTTP；和 TCP 一样，QUIC 应该是一个通用的、可重用的协议。它只知道有独立的流（streams），它可以单独处理，而不必知道它们到底是什么。  
与 HTTP/2 的数据帧（DATA frames）非常相似，QUIC 的流帧（STREAM frames）分别跟踪每个流的字节范围。这与 TCP 不同，TCP 只是将所有流数据附加到一个大 blob 中。像以前一样，让我们考虑一下如果 QUIC 数据包2丢失，而 1 和 3 到达会发生什么。与 TCP 类似，数据包1中流1（stream 1）的数据可以直接传递到浏览器。然而，对于数据包3，QUIC 可以比 TCP 更聪明。它查看流1的字节范围，发现这个流帧（STREAM frame）完全遵循流id 1的第一个流帧 STREAM frame（字节 450 跟在字节 449 之后，因此数据中没有字节间隙）。它可以立即将这些数据提供给浏览器进行处理。然而，对于流id 2，QUIC确实看到了一个缺口（它还没有接收到字节0-299，这些字节在丢失的 QUIC 数据包2中）。它将保存该流帧（STREAM frame），直到 QUIC 数据包2的重传（retransmission）到达。再次将其与 TCP 进行对比，后者也将数据流1的数据保留在数据包3中！  
类似的情况发生在另一种情形下，数据包1丢失，但2和3到达。QUIC 知道它已经接收到流2（stream 2）的所有预期数据，并将其传递给浏览器，只保留流1（stream 1）。  
最有影响的是 QUIC 数据可能不再以与发送时完全相同的顺序发送到浏览器。对于 TCP，如果您发送数据包1、2和3，它们的内容将以完全相同的顺序发送到浏览器（这就是导致队头阻塞的第一个原因）。然而，对于 QUIC，在上面的第二个示例中，在数据包1丢失的情况下，浏览器首先看到数据包2的内容，然后是数据包3的最后一部分，然后是数据包1的（重传），然后是数据包3的第一部分。换言之：QUIC 在单个资源流中保留了顺序，但不再跨单个流（individual streams）进行排序。  
HTTP/2 中的几个系统非常严重地依赖于 TCP 跨流（across streams）的完全确定性排序。例如，HTTP/2 的优先级系统通过传输更改树数据结构（tree data structure ）布局的操作（例如，将资源5添加为资源6的子级）工作的。如果这些操作应用的顺序与发送顺序不同（现在通过 QUIC 是可能出现的），客户端和服务端的优先级状态可能不同。HTTP/2 的头压缩系统 HPACK 也会发生类似的情况。理解这里的细节并不重要，只需要得出结论：要让这些 HTTP/2 系统直接应用 QUIC 是非常困难的。因此，对于 HTTP/3，有些系统使用完全不同的方法。例如，QPACK 是 HTTP/3 的 HPACK 版本，它允许在潜在的队头阻塞和压缩性能之间进行自我选择的权衡。HTTP/2 的优先级系统甚至被完全删除，很可能会被 HTTP/3 的简化版本所取代。所有这些都是因为，与 TCP 不同，QUIC 不能完全保证首先发送的数据也会首先被接收。  
## QUIC 和 HTTP/3 真的完全消除了队头阻塞
它基本上是这样说的：如果你有一个 JavaScript 文件，该文件需要重新组装（re-assembled），就像它是由开发人员创建的一样（或者，老实说，通过 webpack），否则代码将无法工作。任何类型的文件都是一样的：把图片随机地放在一起意味着你阿姨寄来的一些奇怪的电子圣诞卡（甚至更奇怪的）。这意味着，即使在QUIC中，我们仍然有一种队头阻塞的形式：如果在单个流中有一个字节间隙，那么流的后面部分仍然会被阻塞，直到这个间隙被填满。  
这有一个关键的含义：QUIC 的队头阻塞移除只有在多个资源流同时活动时才有效。这样，如果其中一个流上有包丢失，其他流仍然可以继续。如果在某一时刻只有一个流在活动，任何丢包都会影响到这条孤独的流，我们仍然会被阻塞，即使在 QUIC。所以，真正的问题是：我们会经常有多个并发流（simultaneous streams）吗？  
正如对 HTTP/2 所解释的，这是可以通过使用适当的资源调度器/多路复用方法来配置的。流1和流2可以被发送 1122、2121、1221 等，并且浏览器可以使用优先级系统指定它希望服务器遵循的方案（对于 HTTP/3 仍然如此）。所以浏览器可以说：嘿！我注意到这个连接有严重的数据包丢失。我将让服务器以 121212 模式而不是 111222 向我发送资源。这样，如果1的一个数据包丢失，2仍然可以继续工作。然而，这种模式的问题是，121212 模式（或者类似的）对资源加载性能通常不是最优的。  
于最佳性能，有两个相互冲突的性能优化建议：  
- 从 QUIC 的队头阻塞移除中获利：多路复用发送资源（12121212）
- 为了确保浏览器能够尽快处理核心资源：按顺序发送资源（11112222）

假设在下载2个流（绿色和紫色）时，有一个8个丢失包的突发事件，QUIC 对消除队头阻塞确实没有那么大的帮助：在丢包之后收到的绿包不能被浏览器处理，因为它们属于经历丢包的同一个流。第二个（紫色）流的数据尚未收到，因此无法处理。  
这与中间一行不同，中间一行（偶然！）丢失的8个数据包都来自绿色流。这意味着浏览器可以处理最后收到的紫色数据包。然而，正如前面所讨论的，如果浏览器是 JS 或 CSS 文件，如果有更多的紫色数据出现，浏览器可能不会从中受益太多。因此，在这里，我们从 QUIC 的队头阻塞移除中获得了一些好处（因为紫色没有被绿色阻止），但是可能会牺牲整体资源加载性能（因为多路复用会导致文件稍后完成）。  
8个丢失的数据包分布在两个流中。这意味着这两个流现在都被队头阻塞了：不是因为它们像TCP那样在等待对方，而是因为每个流仍然需要自己排序。  
注意：这也是为什么大多数 QUIC 实现很少同时创建包含来自多个流（streams）的数据包（packets）的原因。如果其中一个数据包丢失，则会立即导致单个数据包中所有流的队头阻塞！  
我们看到可能存在某种最佳位置（中间一行），在这里，队头阻塞预防和资源加载性能之间的权衡可能是值得的。然而，正如我们所说，丢包模式很难预测。不会总是8个数据包。它们不会总是一样的8个数据包。  
QUIC 消除队头阻塞可能实际上对 Web 性能没有太大帮助，因为理想情况下，您不希望为了资源加载性能而对许多流进行多路复用。而且，如果你真的想让它工作得很好，你就必须非常巧妙地调整你的多路复用方式来适应连接类型，因为你绝对不想在包丢失非常低的快速网络上进行大量的多路复用（因为它们无论如何都不会遭受太多的队头阻塞）  
原文:  
[QUIC 和 HTTP/3 队头阻塞的细节](https://mp.weixin.qq.com/s/-Z5ei-zXMfjPHUrQZs87ag)